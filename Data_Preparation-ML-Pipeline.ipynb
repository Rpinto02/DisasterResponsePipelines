{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is the pre construct of the ML pipeline of the main database\n",
    "\n",
    "Importing the libraries needed and the dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(19883,)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from scipy.sparse import hstack\n",
    "from sklearn.pipeline import Pipeline\n",
    "import sqlite3\n",
    "\n",
    "def load_data():\n",
    "    '''loading the messages database'''\n",
    "    conn = sqlite3.connect('Messages.db')\n",
    "    df = pd.read_sql('SELECT * FROM Messages', conn)\n",
    "    df = df.drop(columns=['index'])\n",
    "    X = df['message'].values\n",
    "    y= df[df.columns.difference(['message','genre_news','genre_social'])]\n",
    "    return X,y;\n",
    "\n",
    "X, y = load_data()\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(19883, 34)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At a first glance it seems better to treat each message as a document and build a document-term matrix, we may however end up with a matrix with too many columns, but we'll evaluate this later on. But first we'll clean the text: Normalize followed by tokenize then removing stop words and finally lemmatize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = stopwords.words(\"english\")\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def tokenize(text):\n",
    "    # normalize case, remove punctuation and numbers\n",
    "    text = re.sub(r\"[^a-zA-Z]\", \" \", text.lower())\n",
    "    \n",
    "    # tokenize text\n",
    "    tokens = word_tokenize(text)\n",
    "    \n",
    "    # lemmatize and remove stop words\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens if word not in stop_words]\n",
    "    \n",
    "    #lemmatize verbs\n",
    "    tokens = [lemmatizer.lemmatize(word, pos='v') for word in tokens]\n",
    "    \n",
    "    #lemmatize adjectives\n",
    "    tokens = [lemmatizer.lemmatize(word, pos='a') for word in tokens]\n",
    "    \n",
    "    #lemmatize adverbs\n",
    "    tokens = [lemmatizer.lemmatize(word, pos='r') for word in tokens]\n",
    "    \n",
    "    \n",
    "\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_pipeline():\n",
    "    pipeline = Pipeline([\n",
    "        ('vect',CountVectorizer(tokenizer=tokenize)),\n",
    "        ('tfidf',TfidfTransformer()),\n",
    "        ('clf', MultiOutputClassifier(estimator=RandomForestClassifier()))\n",
    "    ])\n",
    "    \n",
    "    return pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('vect',\n",
       "                 CountVectorizer(analyzer='word', binary=False,\n",
       "                                 decode_error='strict',\n",
       "                                 dtype=<class 'numpy.int64'>, encoding='utf-8',\n",
       "                                 input='content', lowercase=True, max_df=1.0,\n",
       "                                 max_features=None, min_df=1,\n",
       "                                 ngram_range=(1, 1), preprocessor=None,\n",
       "                                 stop_words=None, strip_accents=None,\n",
       "                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                 tokenizer=<function tokenize at...\n",
       "                                                                        ccp_alpha=0.0,\n",
       "                                                                        class_weight=None,\n",
       "                                                                        criterion='gini',\n",
       "                                                                        max_depth=None,\n",
       "                                                                        max_features='auto',\n",
       "                                                                        max_leaf_nodes=None,\n",
       "                                                                        max_samples=None,\n",
       "                                                                        min_impurity_decrease=0.0,\n",
       "                                                                        min_impurity_split=None,\n",
       "                                                                        min_samples_leaf=1,\n",
       "                                                                        min_samples_split=2,\n",
       "                                                                        min_weight_fraction_leaf=0.0,\n",
       "                                                                        n_estimators=100,\n",
       "                                                                        n_jobs=None,\n",
       "                                                                        oob_score=False,\n",
       "                                                                        random_state=None,\n",
       "                                                                        verbose=0,\n",
       "                                                                        warm_start=False),\n",
       "                                       n_jobs=None))],\n",
       "         verbose=False)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "random_state=42\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y,test_size=0.2,random_state=random_state)\n",
    "\n",
    "model = model_pipeline()\n",
    "model.fit(X_train, y_train)\n",
    "#y_pred = model.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean AUC:  0.6069162910334058 Max AUC: 0.897529718541132 Min AUC: 0.49933704587642536\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "auc = []\n",
    "for i in range (0,y_test.shape[1]):\n",
    "    auc.append(roc_auc_score(y_test.iloc[:,i],y_pred[:,i]))\n",
    "    \n",
    "\n",
    "import statistics\n",
    "print('Mean AUC: ',statistics.mean(auc),'Max AUC:', max(auc),'Min AUC:', min (auc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3917    1]\n",
      " [  59    0]]\n",
      "[[1206  672]\n",
      " [ 417 1682]]\n"
     ]
    }
   ],
   "source": [
    "cm_y1 = confusion_matrix(y_test.iloc[:,0],y_pred[:,0])\n",
    "cm_y2 = confusion_matrix(y_test.iloc[:,1],y_pred[:,1])\n",
    "#X = document_term_matrix(df['message'])\n",
    "print(cm_y1)\n",
    "print(cm_y2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      1.00      0.99      3918\n",
      "           1       0.00      0.00      0.00        59\n",
      "\n",
      "    accuracy                           0.98      3977\n",
      "   macro avg       0.49      0.50      0.50      3977\n",
      "weighted avg       0.97      0.98      0.98      3977\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.74      0.64      0.69      1878\n",
      "           1       0.71      0.80      0.76      2099\n",
      "\n",
      "    accuracy                           0.73      3977\n",
      "   macro avg       0.73      0.72      0.72      3977\n",
      "weighted avg       0.73      0.73      0.72      3977\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "cr_y1 = classification_report(y_test.iloc[:,0],y_pred[:,0])\n",
    "cr_y2 = classification_report(y_test.iloc[:,1],y_pred[:,1])\n",
    "\n",
    "print(cr_y1)\n",
    "print(cr_y2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#adding the genre dummies columns to the document term matrix we have just created\n",
    "#X_train = hstack((X,np.array(df['genre_news'])[:,None]))\n",
    "#X_train = hstack((X_train,np.array(df['genre_social'])[:,None]))\n",
    "#X_train.shape#old shape 19930,23371"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating our target dataframe\n",
    "#y =  df.drop(columns=['genre_news','genre_social','message'])\n",
    "#y.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
