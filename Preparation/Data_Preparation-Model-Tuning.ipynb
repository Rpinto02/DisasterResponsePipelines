{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is aimed to find better parameters for the evalutaion model.\n",
    "For details on the construction and  decision making process take a look at the ML-Pipeline notebook.\n",
    "\n",
    "\n",
    "Importing the libraries needed and the dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "import sqlite3\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import recall_score\n",
    "import statistics\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def load_data():\n",
    "    '''loading the messages database'''\n",
    "    \n",
    "    #opening the connect and reading the database\n",
    "    conn = sqlite3.connect('Messages.db')\n",
    "    df = pd.read_sql('SELECT * FROM Messages', conn)\n",
    "    df = df.drop(columns=['index'])\n",
    "    \n",
    "    #storing the database into X,y\n",
    "    X = df['message'].values#first scenario will ignore the genre feature\n",
    "    y= df[df.columns.difference(['message','genre_news','genre_social'])]\n",
    "    \n",
    "    #closing connection\n",
    "    conn.close()\n",
    "    \n",
    "    return X,y;\n",
    "\n",
    "\n",
    "\n",
    "X, y = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = stopwords.words(\"english\")\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def tokenize(text):\n",
    "    # normalize case, remove punctuation and numbers\n",
    "    text = re.sub(r\"[^a-zA-Z]\", \" \", text.lower())\n",
    "    \n",
    "    # tokenize text\n",
    "    tokens = word_tokenize(text)\n",
    "    \n",
    "    # lemmatize and remove stop words\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens if word not in stop_words]\n",
    "    \n",
    "    #lemmatize verbs\n",
    "    tokens = [lemmatizer.lemmatize(word, pos='v') for word in tokens]\n",
    "    \n",
    "    #lemmatize adjectives\n",
    "    tokens = [lemmatizer.lemmatize(word, pos='a') for word in tokens]\n",
    "    \n",
    "    #lemmatize adverbs\n",
    "    tokens = [lemmatizer.lemmatize(word, pos='r') for word in tokens]\n",
    "    \n",
    "    \n",
    "\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_pipeline():\n",
    "    '''Pipeline for a model with the default parameters'''\n",
    "    \n",
    "    pipeline = Pipeline([\n",
    "        ('vect',CountVectorizer(tokenizer=tokenize)),\n",
    "        ('tfidf',TfidfTransformer()),\n",
    "        ('clf', MultiOutputClassifier(estimator=RandomForestClassifier()))\n",
    "    ])\n",
    "\n",
    "    # specify parameters for grid search\n",
    "    parameters = {\n",
    "            #'vect__ngram_range': ((1, 1), (1, 2)),\n",
    "            #'vect__max_df': (0.5, 0.75, 1.0),\n",
    "            #'vect__max_features': (None, 5000, 10000),\n",
    "            #'tfidf__use_idf': (True, False),\n",
    "            'clf__estimator__n_estimators': [150],\n",
    "            'clf__estimator__max_depth': [220],\n",
    "            'clf__estimator__random_state': [42]\n",
    "        \n",
    "    }\n",
    "\n",
    "    \n",
    "\n",
    "    # create grid search object\n",
    "    cv = GridSearchCV(pipeline, param_grid=parameters,verbose=1,n_jobs=3)\n",
    "    \n",
    "    return cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=3)]: Using backend LokyBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done   5 out of   5 | elapsed: 18.3min finished\n"
     ]
    }
   ],
   "source": [
    "random_state=42\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y,test_size=0.2,random_state=random_state)\n",
    "\n",
    "model = model_pipeline()\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "def AUC_ROC(y_test,y_pred):\n",
    "    '''Calculates the area under the ROC curve for every label and returns the list\n",
    "    Also displays the mean, maximum and minimum values.\n",
    "    '''\n",
    "    \n",
    "    auc = []\n",
    "    for i in range (0,y_test.shape[1]):\n",
    "        auc.append(roc_auc_score(y_test.iloc[:,i],y_pred[:,i]))\n",
    "    \n",
    "    print('Mean AUC: ',\"%.2f\" % statistics.mean(auc),'Max AUC:', \"%.2f\" % max(auc),'Min AUC:', \"%.2f\" % min (auc))\n",
    "    return auc;\n",
    "\n",
    "\n",
    "\n",
    "def f1_score_labels(y_test,y_pred):\n",
    "    '''Calculates the f1 score for every label, displays it and returns the list\n",
    "    Also displays the mean, maximum and minimum values.\n",
    "    '''\n",
    "    \n",
    "    f1_score_model = []\n",
    "    for i in range (0,y_test.shape[1]):\n",
    "        f1_score_column = f1_score(y_test.iloc[:,i],y_pred[:,i])\n",
    "        f1_score_model.append(f1_score_column)\n",
    "        print('The f1 score for',y.columns[i],' was: ',\"%.2f\" % f1_score_column,'.')\n",
    "    \n",
    "    print('Mean f1 score: ',\"%.2f\" % statistics.mean(f1_score_model),'Max f1 score:',\"%.2f\" % max(f1_score_model),'Min f1 score:',\"%.2f\" % min (f1_score_model))\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "def precision_score_labels(y_test,y_pred):\n",
    "    '''Calculates the precision score for every label, displays it and returns the list\n",
    "    Also displays the mean, maximum and minimum values.\n",
    "    '''\n",
    "    \n",
    "    precision_score_model = []\n",
    "    for i in range (0,y_test.shape[1]):\n",
    "        precision_score_column = precision_score(y_test.iloc[:,i],y_pred[:,i])\n",
    "        precision_score_model.append(precision_score_column)\n",
    "        print('The precision score for',y.columns[i],' was: ',\"%.2f\" % precision_score_column,'.')\n",
    "    \n",
    "    print('Mean precision score: ',\"%.2f\" % statistics.mean(precision_score_model),'Max precision score:',\"%.2f\" % max(precision_score_model),'Min precision score:',\"%.2f\" % min (precision_score_model))\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "def accuracy_score_labels (y_test,y_pred):\n",
    "    '''Calculates the accuracy score for every label, displays it and returns the list\n",
    "    Also displays the mean, maximum and minimum values.\n",
    "    '''\n",
    "    \n",
    "    accuracy_score_model = []\n",
    "    for i in range (0,y_test.shape[1]):\n",
    "        accuracy_score_column = accuracy_score(y_test.iloc[:,i],y_pred[:,i])\n",
    "        accuracy_score_model.append(accuracy_score_column)\n",
    "        print('The accuracy score for',y.columns[i],' was: ',\"%.2f\" % accuracy_score_column,'.')\n",
    "    \n",
    "    print('Mean accuracy score: ',\"%.2f\" % statistics.mean(accuracy_score_model),'Max accuracy score:',\"%.2f\" % max(accuracy_score_model),'Min accuracy score:',\"%.2f\" % min (accuracy_score_model))\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "def recall_score_labels (y_test,y_pred):\n",
    "\n",
    "    recall_score_model = []\n",
    "    for i in range (0,y_test.shape[1]):\n",
    "        recall_score_column = recall_score(y_test.iloc[:,i],y_pred[:,i])\n",
    "        recall_score_model.append(recall_score_column)\n",
    "        print('The recall score for',y.columns[i],' was: ',\"%.2f\" % recall_score_column,'.')\n",
    "    \n",
    "    print('Mean recall score: ',\"%.2f\" % statistics.mean(recall_score_model),'Max recall score:',\"%.2f\" % max(recall_score_model),'Min recall score:',\"%.2f\" % min (recall_score_model))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean AUC:  0.61 Max AUC: 0.89 Min AUC: 0.50\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.4998723838693211,\n",
       " 0.7283671518614524,\n",
       " 0.5809552174697049,\n",
       " 0.5828195907183835,\n",
       " 0.5745079572073704,\n",
       " 0.6485529579794032,\n",
       " 0.694093046771085,\n",
       " 0.8910624667545504,\n",
       " 0.5248212815452775,\n",
       " 0.5084931060879359,\n",
       " 0.7541051945402909,\n",
       " 0.8517406142984758,\n",
       " 0.5,\n",
       " 0.4995904995904996,\n",
       " 0.5692454181560169,\n",
       " 0.5625456243890972,\n",
       " 0.5912776991168824,\n",
       " 0.5079365079365079,\n",
       " 0.5244841372194997,\n",
       " 0.5,\n",
       " 0.5187232576106577,\n",
       " 0.4996022275258552,\n",
       " 0.5249435561811495,\n",
       " 0.5438805685395401,\n",
       " 0.7757916609959696,\n",
       " 0.527841615869785,\n",
       " 0.4998710343048749,\n",
       " 0.7177773632699447,\n",
       " 0.5,\n",
       " 0.7947708154608711,\n",
       " 0.5,\n",
       " 0.5623849459158422,\n",
       " 0.7489759892468127,\n",
       " 0.8423310288521912]"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "AUC_ROC(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The f1 score for aid_centers  was:  0.00 .\n",
      "The f1 score for aid_related  was:  0.76 .\n",
      "The f1 score for buildings  was:  0.27 .\n",
      "The f1 score for clothing  was:  0.27 .\n",
      "The f1 score for cold  was:  0.26 .\n",
      "The f1 score for death  was:  0.44 .\n",
      "The f1 score for direct_report  was:  0.55 .\n",
      "The f1 score for earthquake  was:  0.84 .\n",
      "The f1 score for electricity  was:  0.09 .\n",
      "The f1 score for fire  was:  0.03 .\n",
      "The f1 score for floods  was:  0.66 .\n",
      "The f1 score for food  was:  0.78 .\n",
      "The f1 score for hospitals  was:  0.00 .\n",
      "The f1 score for infrastructure_related  was:  0.00 .\n",
      "The f1 score for medical_help  was:  0.24 .\n",
      "The f1 score for medical_products  was:  0.22 .\n",
      "The f1 score for military  was:  0.29 .\n",
      "The f1 score for missing_people  was:  0.03 .\n",
      "The f1 score for money  was:  0.09 .\n",
      "The f1 score for offer  was:  0.00 .\n",
      "The f1 score for other_aid  was:  0.08 .\n",
      "The f1 score for other_infrastructure  was:  0.00 .\n",
      "The f1 score for other_weather  was:  0.10 .\n",
      "The f1 score for refugees  was:  0.16 .\n",
      "The f1 score for request  was:  0.68 .\n",
      "The f1 score for search_and_rescue  was:  0.11 .\n",
      "The f1 score for security  was:  0.00 .\n",
      "The f1 score for shelter  was:  0.57 .\n",
      "The f1 score for shops  was:  0.00 .\n",
      "The f1 score for storm  was:  0.69 .\n",
      "The f1 score for tools  was:  0.00 .\n",
      "The f1 score for transport  was:  0.22 .\n",
      "The f1 score for water  was:  0.64 .\n",
      "The f1 score for weather_related  was:  0.80 .\n",
      "Mean f1 score:  0.29 Max f1 score: 0.84 Min f1 score: 0.00\n"
     ]
    }
   ],
   "source": [
    "f1_score_labels(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "f1_score with 0 values indicates us that the labels are imbalanced, conducting a grid search will help us get further insights about this behaviour."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The precision score for aid_centers  was:  0.00 .\n",
      "The precision score for aid_related  was:  0.72 .\n",
      "The precision score for buildings  was:  0.77 .\n",
      "The precision score for clothing  was:  0.78 .\n",
      "The precision score for cold  was:  0.89 .\n",
      "The precision score for death  was:  0.78 .\n",
      "The precision score for direct_report  was:  0.72 .\n",
      "The precision score for earthquake  was:  0.90 .\n",
      "The precision score for electricity  was:  0.67 .\n",
      "The precision score for fire  was:  0.50 .\n",
      "The precision score for floods  was:  0.91 .\n",
      "The precision score for food  was:  0.83 .\n",
      "The precision score for hospitals  was:  0.00 .\n",
      "The precision score for infrastructure_related  was:  0.00 .\n",
      "The precision score for medical_help  was:  0.72 .\n",
      "The precision score for medical_products  was:  0.87 .\n",
      "The precision score for military  was:  0.72 .\n",
      "The precision score for missing_people  was:  1.00 .\n",
      "The precision score for money  was:  0.56 .\n",
      "The precision score for offer  was:  0.00 .\n",
      "The precision score for other_aid  was:  0.62 .\n",
      "The precision score for other_infrastructure  was:  0.00 .\n",
      "The precision score for other_weather  was:  0.64 .\n",
      "The precision score for refugees  was:  0.56 .\n",
      "The precision score for request  was:  0.79 .\n",
      "The precision score for search_and_rescue  was:  0.89 .\n",
      "The precision score for security  was:  0.00 .\n",
      "The precision score for shelter  was:  0.78 .\n",
      "The precision score for shops  was:  0.00 .\n",
      "The precision score for storm  was:  0.80 .\n",
      "The precision score for tools  was:  0.00 .\n",
      "The precision score for transport  was:  0.81 .\n",
      "The precision score for water  was:  0.89 .\n",
      "The precision score for weather_related  was:  0.85 .\n",
      "Mean precision score:  0.59 Max precision score: 1.00 Min precision score: 0.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ricardo_2\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\metrics\\_classification.py:1268: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "precision_score_labels(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy score for aid_centers  was:  0.98 .\n",
      "The accuracy score for aid_related  was:  0.73 .\n",
      "The accuracy score for buildings  was:  0.94 .\n",
      "The accuracy score for clothing  was:  0.98 .\n",
      "The accuracy score for cold  was:  0.98 .\n",
      "The accuracy score for death  was:  0.96 .\n",
      "The accuracy score for direct_report  was:  0.82 .\n",
      "The accuracy score for earthquake  was:  0.96 .\n",
      "The accuracy score for electricity  was:  0.97 .\n",
      "The accuracy score for fire  was:  0.99 .\n",
      "The accuracy score for floods  was:  0.95 .\n",
      "The accuracy score for food  was:  0.94 .\n",
      "The accuracy score for hospitals  was:  0.99 .\n",
      "The accuracy score for infrastructure_related  was:  0.92 .\n",
      "The accuracy score for medical_help  was:  0.90 .\n",
      "The accuracy score for medical_products  was:  0.94 .\n",
      "The accuracy score for military  was:  0.97 .\n",
      "The accuracy score for missing_people  was:  0.98 .\n",
      "The accuracy score for money  was:  0.98 .\n",
      "The accuracy score for offer  was:  0.99 .\n",
      "The accuracy score for other_aid  was:  0.83 .\n",
      "The accuracy score for other_infrastructure  was:  0.95 .\n",
      "The accuracy score for other_weather  was:  0.93 .\n",
      "The accuracy score for refugees  was:  0.96 .\n",
      "The accuracy score for request  was:  0.88 .\n",
      "The accuracy score for search_and_rescue  was:  0.97 .\n",
      "The accuracy score for security  was:  0.97 .\n",
      "The accuracy score for shelter  was:  0.93 .\n",
      "The accuracy score for shops  was:  0.99 .\n",
      "The accuracy score for storm  was:  0.93 .\n",
      "The accuracy score for tools  was:  0.99 .\n",
      "The accuracy score for transport  was:  0.95 .\n",
      "The accuracy score for water  was:  0.96 .\n",
      "The accuracy score for weather_related  was:  0.86 .\n",
      "Mean accuracy score:  0.94 Max accuracy score: 0.99 Min accuracy score: 0.73\n"
     ]
    }
   ],
   "source": [
    "accuracy_score_labels (y_test,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The recall score for aid_centers  was:  0.00 .\n",
      "The recall score for aid_related  was:  0.81 .\n",
      "The recall score for buildings  was:  0.17 .\n",
      "The recall score for clothing  was:  0.17 .\n",
      "The recall score for cold  was:  0.15 .\n",
      "The recall score for death  was:  0.30 .\n",
      "The recall score for direct_report  was:  0.44 .\n",
      "The recall score for earthquake  was:  0.79 .\n",
      "The recall score for electricity  was:  0.05 .\n",
      "The recall score for fire  was:  0.02 .\n",
      "The recall score for floods  was:  0.51 .\n",
      "The recall score for food  was:  0.73 .\n",
      "The recall score for hospitals  was:  0.00 .\n",
      "The recall score for infrastructure_related  was:  0.00 .\n",
      "The recall score for medical_help  was:  0.15 .\n",
      "The recall score for medical_products  was:  0.13 .\n",
      "The recall score for military  was:  0.19 .\n",
      "The recall score for missing_people  was:  0.02 .\n",
      "The recall score for money  was:  0.05 .\n",
      "The recall score for offer  was:  0.00 .\n",
      "The recall score for other_aid  was:  0.04 .\n",
      "The recall score for other_infrastructure  was:  0.00 .\n",
      "The recall score for other_weather  was:  0.05 .\n",
      "The recall score for refugees  was:  0.09 .\n",
      "The recall score for request  was:  0.59 .\n",
      "The recall score for search_and_rescue  was:  0.06 .\n",
      "The recall score for security  was:  0.00 .\n",
      "The recall score for shelter  was:  0.45 .\n",
      "The recall score for shops  was:  0.00 .\n",
      "The recall score for storm  was:  0.61 .\n",
      "The recall score for tools  was:  0.00 .\n",
      "The recall score for transport  was:  0.13 .\n",
      "The recall score for water  was:  0.50 .\n",
      "The recall score for weather_related  was:  0.76 .\n",
      "Mean recall score:  0.23 Max recall score: 0.81 Min recall score: 0.00\n"
     ]
    }
   ],
   "source": [
    "recall_score_labels (y_test,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ricardo_2\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\metrics\\_classification.py:1268: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "cm_y1 = confusion_matrix(y_test.iloc[:,0],y_pred[:,0])\n",
    "cm_y2 = confusion_matrix(y_test.iloc[:,1],y_pred[:,1])\n",
    "\n",
    "\n",
    "\n",
    "cr_y0 = classification_report(y_test.iloc[:,0],y_pred[:,0])\n",
    "cr_y9 = classification_report(y_test.iloc[:,9],y_pred[:,9])\n",
    "cr_y13 = classification_report(y_test.iloc[:,13],y_pred[:,13])\n",
    "cr_y19 = classification_report(y_test.iloc[:,19],y_pred[:,19])\n",
    "cr_y21 = classification_report(y_test.iloc[:,21],y_pred[:,21])\n",
    "cr_y26 = classification_report(y_test.iloc[:,26],y_pred[:,26])\n",
    "cr_y28 = classification_report(y_test.iloc[:,28],y_pred[:,28])\n",
    "cr_y30 = classification_report(y_test.iloc[:,30],y_pred[:,30])\n",
    "cr_y31 = classification_report(y_test.iloc[:,31],y_pred[:,31])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      1.00      0.97      3748\n",
      "           1       0.81      0.13      0.22       229\n",
      "\n",
      "    accuracy                           0.95      3977\n",
      "   macro avg       0.88      0.56      0.60      3977\n",
      "weighted avg       0.94      0.95      0.93      3977\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print (cr_y31)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'clf__estimator__max_depth': 220,\n",
       " 'clf__estimator__n_estimators': 150,\n",
       " 'clf__estimator__random_state': 42}"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far the parameters tested max_depth: 5,6, 50, 100, 150 200, 220, 250: 220 was the best one. And the estimators: 50,100,150: 150 proved to be best. However from the already implemeted model there is no significant difference so there is no reason to overwrite for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
