{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is aimed to find better parameters for the evalutaion model.\n",
    "For details on the construction and  decision making process take a look at the ML-Pipeline notebook.\n",
    "\n",
    "\n",
    "Importing the libraries needed and the dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "import sqlite3\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import recall_score\n",
    "import statistics\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def load_data():\n",
    "    '''loading the messages database'''\n",
    "    \n",
    "    #opening the connect and reading the database\n",
    "    conn = sqlite3.connect('Messages.db')\n",
    "    df = pd.read_sql('SELECT * FROM Messages', conn)\n",
    "    df = df.drop(columns=['index'])\n",
    "    \n",
    "    #storing the database into X,y\n",
    "    X = df['message'].values#first scenario will ignore the genre feature\n",
    "    y= df[df.columns.difference(['message','genre_news','genre_social'])]\n",
    "    \n",
    "    #closing connection\n",
    "    conn.close()\n",
    "    \n",
    "    return X,y;\n",
    "\n",
    "\n",
    "\n",
    "X, y = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = stopwords.words(\"english\")\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def tokenize(text):\n",
    "    # normalize case, remove punctuation and numbers\n",
    "    text = re.sub(r\"[^a-zA-Z]\", \" \", text.lower())\n",
    "    \n",
    "    # tokenize text\n",
    "    tokens = word_tokenize(text)\n",
    "    \n",
    "    # lemmatize and remove stop words\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens if word not in stop_words]\n",
    "    \n",
    "    #lemmatize verbs\n",
    "    tokens = [lemmatizer.lemmatize(word, pos='v') for word in tokens]\n",
    "    \n",
    "    #lemmatize adjectives\n",
    "    tokens = [lemmatizer.lemmatize(word, pos='a') for word in tokens]\n",
    "    \n",
    "    #lemmatize adverbs\n",
    "    tokens = [lemmatizer.lemmatize(word, pos='r') for word in tokens]\n",
    "    \n",
    "    \n",
    "\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_pipeline():\n",
    "    '''Pipeline for a model with the default parameters'''\n",
    "    \n",
    "    pipeline = Pipeline([\n",
    "        ('vect',CountVectorizer(tokenizer=tokenize)),\n",
    "        ('tfidf',TfidfTransformer()),\n",
    "        ('clf', MultiOutputClassifier(estimator=RandomForestClassifier()))\n",
    "    ])\n",
    "\n",
    "    # specify parameters for grid search\n",
    "    parameters = {\n",
    "            #'vect__ngram_range': ((1, 1), (1, 2)),\n",
    "            #'vect__max_df': (0.5, 0.75, 1.0),\n",
    "            #'vect__max_features': (None, 5000, 10000),\n",
    "            #'tfidf__use_idf': (True, False),\n",
    "            'clf__estimator__n_estimators': [100],\n",
    "            #'clf__estimator__max_depth': [220],\n",
    "            'clf__estimator__random_state': [42]\n",
    "        \n",
    "    }\n",
    "\n",
    "    \n",
    "\n",
    "    # create grid search object\n",
    "    cv = GridSearchCV(pipeline, param_grid=parameters,verbose=1,n_jobs=3)\n",
    "    \n",
    "    return cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=3)]: Using backend LokyBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done   5 out of   5 | elapsed: 12.0min finished\n"
     ]
    }
   ],
   "source": [
    "random_state=42\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y,test_size=0.2,random_state=random_state)\n",
    "\n",
    "model = model_pipeline()\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_scores(y_test,y_pred):\n",
    "    '''Calculates the area under the ROC curve,f1_score, accuracy, precision and recall scores for every label\n",
    "    and displays the mean, maximum and minimum values of each score.'''\n",
    "    \n",
    "    auc = []\n",
    "    for i in range (0,y_test.shape[1]):\n",
    "        auc.append(roc_auc_score(y_test.iloc[:,i],y_pred[:,i]))\n",
    "    print('Mean AUC:',\"%.2f\" % statistics.mean(auc),' Max AUC:', \"%.2f\" % max(auc),' Min AUC:', \"%.2f\" % min (auc))\n",
    "    \n",
    "    \n",
    "    f1_score_model = []\n",
    "    for i in range (0,y_test.shape[1]):\n",
    "        f1_score_column = f1_score(y_test.iloc[:,i],y_pred[:,i])\n",
    "        f1_score_model.append(f1_score_column)\n",
    "    print('Mean f1 score:',\"%.2f\" % statistics.mean(f1_score_model),' Max f1 score:',\"%.2f\" % max(f1_score_model),' Min f1 score:',\"%.2f\" % min (f1_score_model))\n",
    "    \n",
    "    \n",
    "    precision_score_model = []\n",
    "    for i in range (0,y_test.shape[1]):\n",
    "        precision_score_column = precision_score(y_test.iloc[:,i],y_pred[:,i])\n",
    "        precision_score_model.append(precision_score_column)\n",
    "    print('Mean precision score:',\"%.2f\" % statistics.mean(precision_score_model),' Max precision score:',\"%.2f\" % max(precision_score_model),' Min precision score:',\"%.2f\" % min (precision_score_model))\n",
    "    \n",
    "    accuracy_score_model = []\n",
    "    for i in range (0,y_test.shape[1]):\n",
    "        accuracy_score_column = accuracy_score(y_test.iloc[:,i],y_pred[:,i])\n",
    "        accuracy_score_model.append(accuracy_score_column)\n",
    "    print('Mean accuracy score:',\"%.2f\" % statistics.mean(accuracy_score_model),' Max accuracy score:',\"%.2f\" % max(accuracy_score_model),' Min accuracy score:',\"%.2f\" % min (accuracy_score_model))\n",
    "    \n",
    "    recall_score_model = []\n",
    "    for i in range (0,y_test.shape[1]):\n",
    "        recall_score_column = recall_score(y_test.iloc[:,i],y_pred[:,i])\n",
    "        recall_score_model.append(recall_score_column)\n",
    "    print('Mean recall score:',\"%.2f\" % statistics.mean(recall_score_model),' Max recall score:',\"%.2f\" % max(recall_score_model),' Min recall score:',\"%.2f\" % min (recall_score_model))\n",
    "    \n",
    "    \n",
    "\n",
    "def AUC_ROC(y_test,y_pred):\n",
    "    '''Calculates the area under the ROC curve for every label and displays the value.\n",
    "    '''\n",
    "    \n",
    "    auc = []\n",
    "    for i in range (0,y_test.shape[1]):\n",
    "        auc_score_column = roc_auc_score(y_test.iloc[:,i],y_pred[:,i])\n",
    "        auc.append(auc_score_column)\n",
    "        print('The AUC for',y.columns[i],' was: ',\"%.2f\" % auc_score_column,'.')\n",
    "    \n",
    "    \n",
    "    #return auc; remove the comment if you want to return the list\n",
    "\n",
    "\n",
    "\n",
    "def f1_score_labels(y_test,y_pred):\n",
    "    '''Calculates the f1 score for every label and displays the value.\n",
    "    '''\n",
    "    \n",
    "    f1_score_model = []\n",
    "    for i in range (0,y_test.shape[1]):\n",
    "        f1_score_column = f1_score(y_test.iloc[:,i],y_pred[:,i])\n",
    "        f1_score_model.append(f1_score_column)\n",
    "        print('The f1 score for',y.columns[i],' was: ',\"%.2f\" % f1_score_column,'.')\n",
    "    \n",
    "    #return f1_score_model; remove the comment if you want to return the list\n",
    "\n",
    "\n",
    "\n",
    "def precision_score_labels(y_test,y_pred):\n",
    "    '''Calculates the precision score for every label and displays the value.\n",
    "    '''\n",
    "    \n",
    "    precision_score_model = []\n",
    "    for i in range (0,y_test.shape[1]):\n",
    "        precision_score_column = precision_score(y_test.iloc[:,i],y_pred[:,i])\n",
    "        precision_score_model.append(precision_score_column)\n",
    "        print('The precision score for',y.columns[i],' was: ',\"%.2f\" % precision_score_column,'.')\n",
    "    \n",
    "    #return precision_score_model; remove the comment if you want to return the list\n",
    "\n",
    "\n",
    "def accuracy_score_labels (y_test,y_pred):\n",
    "    '''Calculates the accuracy score for every label and displays the value.\n",
    "    '''\n",
    "    \n",
    "    accuracy_score_model = []\n",
    "    for i in range (0,y_test.shape[1]):\n",
    "        accuracy_score_column = accuracy_score(y_test.iloc[:,i],y_pred[:,i])\n",
    "        accuracy_score_model.append(accuracy_score_column)\n",
    "        print('The accuracy score for',y.columns[i],' was: ',\"%.2f\" % accuracy_score_column,'.')\n",
    "    \n",
    "    #return accuracy_score_model; remove the comment if you want to return the list\n",
    "    \n",
    "\n",
    "\n",
    "def recall_score_labels (y_test,y_pred):\n",
    "    '''Calculates the accuracy score for every label and displays the value.\n",
    "    '''\n",
    "\n",
    "    recall_score_model = []\n",
    "    for i in range (0,y_test.shape[1]):\n",
    "        recall_score_column = recall_score(y_test.iloc[:,i],y_pred[:,i])\n",
    "        recall_score_model.append(recall_score_column)\n",
    "        print('The recall score for',y.columns[i],' was: ',\"%.2f\" % recall_score_column,'.')\n",
    "    \n",
    "    #return recall_score_model; remove the comment if you want to return the list\n",
    "\n",
    "\n",
    "\n",
    "#if you want the score values for just one label run the following functions    \n",
    "def AUC_ROC_column(y_test,y_pred, column):\n",
    "    '''Calculates the area under the ROC curve for the column(label) and displays the value.\n",
    "    '''\n",
    "    index = y_test.columns.get_loc(column)\n",
    "    auc_score_column = roc_auc_score(y_test.iloc[:,index],y_pred[:,index])\n",
    "    print('The AUC for',column,'was: ',\"%.2f\" % auc_score_column,'.')\n",
    "    \n",
    "    \n",
    "    #return auc_score_column; remove the comment if you want to return the value\n",
    "    \n",
    "def f1_score_column(y_test,y_pred,column):\n",
    "    '''Calculates the f1 score for the column(label) and displays the value.\n",
    "    '''\n",
    "    \n",
    "    index = y_test.columns.get_loc(column)\n",
    "    f1_score_column = f1_score(y_test.iloc[:,index],y_pred[:,index])\n",
    "    print('The f1 score for',column,'was: ',\"%.2f\" % f1_score_column,'.')\n",
    "    \n",
    "    #return f1_score_column; remove the comment if you want to return the value\n",
    "\n",
    "\n",
    "\n",
    "def precision_score_column(y_test,y_pred,column):\n",
    "    '''Calculates the precision score for the column(label) and displays the value.\n",
    "    '''\n",
    "    \n",
    "    index = y_test.columns.get_loc(column)\n",
    "    precision_score_column = precision_score(y_test.iloc[:,index],y_pred[:,index])  \n",
    "    print('The precision score for',column,'was: ',\"%.2f\" % precision_score_column,'.')\n",
    "    \n",
    "    #return precision_score_column; remove the comment if you want to return the value\n",
    "\n",
    "\n",
    "def accuracy_score_column (y_test,y_pred,column):\n",
    "    '''Calculates the accuracy score for the column(label) and displays the value.\n",
    "    '''\n",
    "    \n",
    "    index = y_test.columns.get_loc(column)\n",
    "    accuracy_score_column = accuracy_score(y_test.iloc[:,index],y_pred[:,index])\n",
    "    print('The accuracy score for',column,'was: ',\"%.2f\" % accuracy_score_column,'.')\n",
    "    \n",
    "    #return accuracy_score_column; remove the comment if you want to return the value\n",
    "    \n",
    "\n",
    "\n",
    "def recall_score_column (y_test,y_pred,column):\n",
    "    '''Calculates the accuracy score for the column(label) and displays the value.\n",
    "    '''\n",
    "\n",
    "    index = y_test.columns.get_loc(column)\n",
    "    recall_score_column = recall_score(y_test.iloc[:,index],y_pred[:,index])\n",
    "    print('The recall score for',column,'was: ',\"%.2f\" % recall_score_column,'.')\n",
    "    \n",
    "    #return recall_score_column; remove the comment if you want to return the value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The AUC for aid_centers  was:  0.50 .\n",
      "The AUC for aid_related  was:  0.73 .\n",
      "The AUC for buildings  was:  0.57 .\n",
      "The AUC for clothing  was:  0.59 .\n",
      "The AUC for cold  was:  0.58 .\n",
      "The AUC for death  was:  0.64 .\n",
      "The AUC for direct_report  was:  0.70 .\n",
      "The AUC for earthquake  was:  0.90 .\n",
      "The AUC for electricity  was:  0.52 .\n",
      "The AUC for fire  was:  0.51 .\n",
      "The AUC for floods  was:  0.76 .\n",
      "The AUC for food  was:  0.85 .\n",
      "The AUC for hospitals  was:  0.51 .\n",
      "The AUC for infrastructure_related  was:  0.50 .\n",
      "The AUC for medical_help  was:  0.57 .\n",
      "The AUC for medical_products  was:  0.55 .\n",
      "The AUC for military  was:  0.58 .\n",
      "The AUC for missing_people  was:  0.51 .\n",
      "The AUC for money  was:  0.53 .\n",
      "The AUC for offer  was:  0.50 .\n",
      "The AUC for other_aid  was:  0.52 .\n",
      "The AUC for other_infrastructure  was:  0.50 .\n",
      "The AUC for other_weather  was:  0.54 .\n",
      "The AUC for refugees  was:  0.55 .\n",
      "The AUC for request  was:  0.78 .\n",
      "The AUC for search_and_rescue  was:  0.52 .\n",
      "The AUC for security  was:  0.50 .\n",
      "The AUC for shelter  was:  0.71 .\n",
      "The AUC for shops  was:  0.50 .\n",
      "The AUC for storm  was:  0.79 .\n",
      "The AUC for tools  was:  0.50 .\n",
      "The AUC for transport  was:  0.58 .\n",
      "The AUC for water  was:  0.76 .\n",
      "The AUC for weather_related  was:  0.84 .\n"
     ]
    }
   ],
   "source": [
    "AUC_ROC(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The f1 score for aid_centers  was:  0.00 .\n",
      "The f1 score for aid_related  was:  0.76 .\n",
      "The f1 score for buildings  was:  0.25 .\n",
      "The f1 score for clothing  was:  0.30 .\n",
      "The f1 score for cold  was:  0.27 .\n",
      "The f1 score for death  was:  0.42 .\n",
      "The f1 score for direct_report  was:  0.56 .\n",
      "The f1 score for earthquake  was:  0.85 .\n",
      "The f1 score for electricity  was:  0.08 .\n",
      "The f1 score for fire  was:  0.03 .\n",
      "The f1 score for floods  was:  0.66 .\n",
      "The f1 score for food  was:  0.77 .\n",
      "The f1 score for hospitals  was:  0.04 .\n",
      "The f1 score for infrastructure_related  was:  0.01 .\n",
      "The f1 score for medical_help  was:  0.24 .\n",
      "The f1 score for medical_products  was:  0.18 .\n",
      "The f1 score for military  was:  0.26 .\n",
      "The f1 score for missing_people  was:  0.03 .\n",
      "The f1 score for money  was:  0.11 .\n",
      "The f1 score for offer  was:  0.00 .\n",
      "The f1 score for other_aid  was:  0.09 .\n",
      "The f1 score for other_infrastructure  was:  0.00 .\n",
      "The f1 score for other_weather  was:  0.14 .\n",
      "The f1 score for refugees  was:  0.18 .\n",
      "The f1 score for request  was:  0.69 .\n",
      "The f1 score for search_and_rescue  was:  0.07 .\n",
      "The f1 score for security  was:  0.00 .\n",
      "The f1 score for shelter  was:  0.57 .\n",
      "The f1 score for shops  was:  0.00 .\n",
      "The f1 score for storm  was:  0.69 .\n",
      "The f1 score for tools  was:  0.00 .\n",
      "The f1 score for transport  was:  0.28 .\n",
      "The f1 score for water  was:  0.66 .\n",
      "The f1 score for weather_related  was:  0.81 .\n"
     ]
    }
   ],
   "source": [
    "f1_score_labels(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The precision score for aid_centers  was:  0.00 .\n",
      "The precision score for aid_related  was:  0.72 .\n",
      "The precision score for buildings  was:  0.76 .\n",
      "The precision score for clothing  was:  0.76 .\n",
      "The precision score for cold  was:  0.89 .\n",
      "The precision score for death  was:  0.80 .\n",
      "The precision score for direct_report  was:  0.72 .\n",
      "The precision score for earthquake  was:  0.90 .\n",
      "The precision score for electricity  was:  0.83 .\n",
      "The precision score for fire  was:  1.00 .\n",
      "The precision score for floods  was:  0.90 .\n",
      "The precision score for food  was:  0.84 .\n",
      "The precision score for hospitals  was:  1.00 .\n",
      "The precision score for infrastructure_related  was:  0.11 .\n",
      "The precision score for medical_help  was:  0.65 .\n",
      "The precision score for medical_products  was:  0.82 .\n",
      "The precision score for military  was:  0.71 .\n",
      "The precision score for missing_people  was:  1.00 .\n",
      "The precision score for money  was:  0.67 .\n",
      "The precision score for offer  was:  0.00 .\n",
      "The precision score for other_aid  was:  0.56 .\n",
      "The precision score for other_infrastructure  was:  0.00 .\n",
      "The precision score for other_weather  was:  0.61 .\n",
      "The precision score for refugees  was:  0.65 .\n",
      "The precision score for request  was:  0.78 .\n",
      "The precision score for search_and_rescue  was:  1.00 .\n",
      "The precision score for security  was:  0.00 .\n",
      "The precision score for shelter  was:  0.78 .\n",
      "The precision score for shops  was:  0.00 .\n",
      "The precision score for storm  was:  0.80 .\n",
      "The precision score for tools  was:  0.00 .\n",
      "The precision score for transport  was:  0.81 .\n",
      "The precision score for water  was:  0.88 .\n",
      "The precision score for weather_related  was:  0.85 .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ricardo_2\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\metrics\\_classification.py:1268: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "precision_score_labels(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy score for aid_centers  was:  0.98 .\n",
      "The accuracy score for aid_related  was:  0.73 .\n",
      "The accuracy score for buildings  was:  0.94 .\n",
      "The accuracy score for clothing  was:  0.98 .\n",
      "The accuracy score for cold  was:  0.98 .\n",
      "The accuracy score for death  was:  0.96 .\n",
      "The accuracy score for direct_report  was:  0.82 .\n",
      "The accuracy score for earthquake  was:  0.96 .\n",
      "The accuracy score for electricity  was:  0.97 .\n",
      "The accuracy score for fire  was:  0.99 .\n",
      "The accuracy score for floods  was:  0.95 .\n",
      "The accuracy score for food  was:  0.94 .\n",
      "The accuracy score for hospitals  was:  0.99 .\n",
      "The accuracy score for infrastructure_related  was:  0.92 .\n",
      "The accuracy score for medical_help  was:  0.90 .\n",
      "The accuracy score for medical_products  was:  0.94 .\n",
      "The accuracy score for military  was:  0.97 .\n",
      "The accuracy score for missing_people  was:  0.98 .\n",
      "The accuracy score for money  was:  0.98 .\n",
      "The accuracy score for offer  was:  0.99 .\n",
      "The accuracy score for other_aid  was:  0.83 .\n",
      "The accuracy score for other_infrastructure  was:  0.95 .\n",
      "The accuracy score for other_weather  was:  0.93 .\n",
      "The accuracy score for refugees  was:  0.96 .\n",
      "The accuracy score for request  was:  0.88 .\n",
      "The accuracy score for search_and_rescue  was:  0.97 .\n",
      "The accuracy score for security  was:  0.97 .\n",
      "The accuracy score for shelter  was:  0.93 .\n",
      "The accuracy score for shops  was:  0.99 .\n",
      "The accuracy score for storm  was:  0.93 .\n",
      "The accuracy score for tools  was:  0.99 .\n",
      "The accuracy score for transport  was:  0.95 .\n",
      "The accuracy score for water  was:  0.96 .\n",
      "The accuracy score for weather_related  was:  0.87 .\n"
     ]
    }
   ],
   "source": [
    "accuracy_score_labels (y_test,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The recall score for aid_centers  was:  0.00 .\n",
      "The recall score for aid_related  was:  0.80 .\n",
      "The recall score for buildings  was:  0.15 .\n",
      "The recall score for clothing  was:  0.19 .\n",
      "The recall score for cold  was:  0.16 .\n",
      "The recall score for death  was:  0.28 .\n",
      "The recall score for direct_report  was:  0.46 .\n",
      "The recall score for earthquake  was:  0.81 .\n",
      "The recall score for electricity  was:  0.04 .\n",
      "The recall score for fire  was:  0.02 .\n",
      "The recall score for floods  was:  0.52 .\n",
      "The recall score for food  was:  0.72 .\n",
      "The recall score for hospitals  was:  0.02 .\n",
      "The recall score for infrastructure_related  was:  0.00 .\n",
      "The recall score for medical_help  was:  0.15 .\n",
      "The recall score for medical_products  was:  0.10 .\n",
      "The recall score for military  was:  0.16 .\n",
      "The recall score for missing_people  was:  0.02 .\n",
      "The recall score for money  was:  0.06 .\n",
      "The recall score for offer  was:  0.00 .\n",
      "The recall score for other_aid  was:  0.05 .\n",
      "The recall score for other_infrastructure  was:  0.00 .\n",
      "The recall score for other_weather  was:  0.08 .\n",
      "The recall score for refugees  was:  0.10 .\n",
      "The recall score for request  was:  0.61 .\n",
      "The recall score for search_and_rescue  was:  0.03 .\n",
      "The recall score for security  was:  0.00 .\n",
      "The recall score for shelter  was:  0.44 .\n",
      "The recall score for shops  was:  0.00 .\n",
      "The recall score for storm  was:  0.61 .\n",
      "The recall score for tools  was:  0.00 .\n",
      "The recall score for transport  was:  0.17 .\n",
      "The recall score for water  was:  0.53 .\n",
      "The recall score for weather_related  was:  0.77 .\n"
     ]
    }
   ],
   "source": [
    "recall_score_labels (y_test,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean AUC: 0.61  Max AUC: 0.90  Min AUC: 0.50\n",
      "Mean f1 score: 0.29  Max f1 score: 0.85  Min f1 score: 0.00\n",
      "Mean precision score: 0.64  Max precision score: 1.00  Min precision score: 0.00\n",
      "Mean accuracy score: 0.94  Max accuracy score: 0.99  Min accuracy score: 0.73\n",
      "Mean recall score: 0.24  Max recall score: 0.81  Min recall score: 0.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ricardo_2\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\metrics\\_classification.py:1268: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "model_scores(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The AUC for aid_centers  was:  0.50 .\n"
     ]
    }
   ],
   "source": [
    "AUC_ROC_column(y_test,y_pred,'aid_centers')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The f1 score for aid_centers was:  0.00 .\n"
     ]
    }
   ],
   "source": [
    "f1_score_column(y_test,y_pred,'aid_centers')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The precision score for aid_centers was:  0.00 .\n"
     ]
    }
   ],
   "source": [
    "precision_score_column(y_test,y_pred,'aid_centers')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy score for aid_centers was:  0.98 .\n"
     ]
    }
   ],
   "source": [
    "accuracy_score_column(y_test,y_pred,'aid_centers')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The recall score for aid_centers was:  0.00 .\n"
     ]
    }
   ],
   "source": [
    "recall_score_column(y_test,y_pred,'aid_centers')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
