{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is the pre construct of the ML pipeline of the main database\n",
    "\n",
    "Importing the libraries needed and the dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "import sqlite3\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import recall_score\n",
    "import statistics\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def load_data():\n",
    "    '''loading the messages database'''\n",
    "    \n",
    "    #opening the connect and reading the database\n",
    "    conn = sqlite3.connect('Messages.db')\n",
    "    df = pd.read_sql('SELECT * FROM Messages', conn)\n",
    "    df = df.drop(columns=['index'])\n",
    "    \n",
    "    #storing the database into X,y\n",
    "    X = df['message'].values#first scenario will ignore the genre feature\n",
    "    y= df[df.columns.difference(['message','genre_news','genre_social'])]\n",
    "    \n",
    "    #closing connection\n",
    "    conn.close()\n",
    "    \n",
    "    return X,y;\n",
    "\n",
    "\n",
    "\n",
    "X, y = load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At a first glance it seems better to treat each message as a document and build a document-term matrix, we may however end up with a matrix with too many columns, but we'll evaluate this later on. But first we'll clean the text: Normalize followed by tokenize then removing stop words and finally lemmatize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = stopwords.words(\"english\")\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def tokenize(text):\n",
    "    # normalize case, remove punctuation and numbers\n",
    "    text = re.sub(r\"[^a-zA-Z]\", \" \", text.lower())\n",
    "    \n",
    "    # tokenize text\n",
    "    tokens = word_tokenize(text)\n",
    "    \n",
    "    # lemmatize and remove stop words\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens if word not in stop_words]\n",
    "    \n",
    "    #lemmatize verbs\n",
    "    tokens = [lemmatizer.lemmatize(word, pos='v') for word in tokens]\n",
    "    \n",
    "    #lemmatize adjectives\n",
    "    tokens = [lemmatizer.lemmatize(word, pos='a') for word in tokens]\n",
    "    \n",
    "    #lemmatize adverbs\n",
    "    tokens = [lemmatizer.lemmatize(word, pos='r') for word in tokens]\n",
    "    \n",
    "    \n",
    "\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_pipeline():\n",
    "    '''Pipeline for a model with the default parameters'''\n",
    "    \n",
    "    pipeline = Pipeline([\n",
    "        ('vect',CountVectorizer(tokenizer=tokenize)),\n",
    "        ('tfidf',TfidfTransformer()),\n",
    "        ('clf', MultiOutputClassifier(estimator=RandomForestClassifier(random_state=42)))\n",
    "    ])\n",
    "    \n",
    "    return pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we look at how the default RandomForestClassifier behaves then we will conduct a grid search to enhance it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_state=42\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y,test_size=0.2,random_state=random_state)\n",
    "\n",
    "model = model_pipeline()\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The evaluation on the model will be based on:\n",
    "f1_score, accuracy, precision, recall and area under the roc curve.\n",
    "Since this is a multi output classifier we need to build functions so we can look at each of this scores per label. These functions are created below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def AUC_ROC(y_test,y_pred):\n",
    "    '''Calculates the area under the ROC curve for every label and returns the list\n",
    "    Also displays the mean, maximum and minimum values.\n",
    "    '''\n",
    "    \n",
    "    auc = []\n",
    "    for i in range (0,y_test.shape[1]):\n",
    "        auc.append(roc_auc_score(y_test.iloc[:,i],y_pred[:,i]))\n",
    "    \n",
    "    print('Mean AUC: ',\"%.2f\" % statistics.mean(auc),'Max AUC:', \"%.2f\" % max(auc),'Min AUC:', \"%.2f\" % min (auc))\n",
    "    return auc;\n",
    "\n",
    "\n",
    "\n",
    "def f1_score_labels(y_test,y_pred):\n",
    "    '''Calculates the f1 score for every label, displays it and returns the list\n",
    "    Also displays the mean, maximum and minimum values.\n",
    "    '''\n",
    "    \n",
    "    f1_score_model = []\n",
    "    for i in range (0,y_test.shape[1]):\n",
    "        f1_score_column = f1_score(y_test.iloc[:,i],y_pred[:,i])\n",
    "        f1_score_model.append(f1_score_column)\n",
    "        print('The f1 score for',y.columns[i],' was: ',\"%.2f\" % f1_score_column,'.')\n",
    "    \n",
    "    print('Mean f1 score: ',\"%.2f\" % statistics.mean(f1_score_model),'Max f1 score:',\"%.2f\" % max(f1_score_model),'Min f1 score:',\"%.2f\" % min (f1_score_model))\n",
    "    return f1_score_model;\n",
    "\n",
    "\n",
    "\n",
    "def precision_score_labels(y_test,y_pred):\n",
    "    '''Calculates the precision score for every label, displays it and returns the list\n",
    "    Also displays the mean, maximum and minimum values.\n",
    "    '''\n",
    "    \n",
    "    precision_score_model = []\n",
    "    for i in range (0,y_test.shape[1]):\n",
    "        precision_score_column = precision_score(y_test.iloc[:,i],y_pred[:,i])\n",
    "        precision_score_model.append(precision_score_column)\n",
    "        print('The precision score for',y.columns[i],' was: ',\"%.2f\" % precision_score_column,'.')\n",
    "    \n",
    "    print('Mean precision score: ',\"%.2f\" % statistics.mean(precision_score_model),'Max precision score:',\"%.2f\" % max(precision_score_model),'Min precision score:',\"%.2f\" % min (precision_score_model))\n",
    "    return precision_score_model;\n",
    "    \n",
    "\n",
    "\n",
    "def accuracy_score_labels (y_test,y_pred):\n",
    "    '''Calculates the accuracy score for every label, displays it and returns the list\n",
    "    Also displays the mean, maximum and minimum values.\n",
    "    '''\n",
    "    \n",
    "    accuracy_score_model = []\n",
    "    for i in range (0,y_test.shape[1]):\n",
    "        accuracy_score_column = accuracy_score(y_test.iloc[:,i],y_pred[:,i])\n",
    "        accuracy_score_model.append(accuracy_score_column)\n",
    "        print('The accuracy score for',y.columns[i],' was: ',\"%.2f\" % accuracy_score_column,'.')\n",
    "    \n",
    "    print('Mean accuracy score: ',\"%.2f\" % statistics.mean(accuracy_score_model),'Max accuracy score:',\"%.2f\" % max(accuracy_score_model),'Min accuracy score:',\"%.2f\" % min (accuracy_score_model))\n",
    "    return accuracy_score_model;\n",
    "\n",
    "\n",
    "\n",
    "def recall_score_labels (y_test,y_pred):\n",
    "\n",
    "    recall_score_model = []\n",
    "    for i in range (0,y_test.shape[1]):\n",
    "        recall_score_column = recall_score(y_test.iloc[:,i],y_pred[:,i])\n",
    "        recall_score_model.append(recall_score_column)\n",
    "        print('The recall score for',y.columns[i],' was: ',\"%.2f\" % recall_score_column,'.')\n",
    "    \n",
    "    print('Mean recall score: ',\"%.2f\" % statistics.mean(recall_score_model),'Max recall score:',\"%.2f\" % max(recall_score_model),'Min recall score:',\"%.2f\" % min (recall_score_model))\n",
    "    return recall_score_model;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean AUC:  0.61 Max AUC: 0.90 Min AUC: 0.50\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.4998723838693211,\n",
       " 0.7250041984595332,\n",
       " 0.5716914558019949,\n",
       " 0.5945959169694078,\n",
       " 0.5791808544036321,\n",
       " 0.6395174020897542,\n",
       " 0.7000298596595999,\n",
       " 0.8970986840583733,\n",
       " 0.5208788025318993,\n",
       " 0.5086206896551724,\n",
       " 0.7576626494337885,\n",
       " 0.8488608594863163,\n",
       " 0.5096153846153846,\n",
       " 0.500500355595897,\n",
       " 0.5702206915940399,\n",
       " 0.5509168182324339,\n",
       " 0.5781633507925902,\n",
       " 0.5079365079365079,\n",
       " 0.5296131029146247,\n",
       " 0.5,\n",
       " 0.5210582591970516,\n",
       " 0.49933704587642536,\n",
       " 0.5390043831683129,\n",
       " 0.5503346688288976,\n",
       " 0.7837048495294779,\n",
       " 0.5174825174825175,\n",
       " 0.4998710343048749,\n",
       " 0.7142396274208881,\n",
       " 0.5,\n",
       " 0.7925445024306548,\n",
       " 0.5,\n",
       " 0.581768791972895,\n",
       " 0.7595723304817448,\n",
       " 0.8443566716013274]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "AUC_ROC(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The f1 score for aid_centers  was:  0.00 .\n",
      "The f1 score for aid_related  was:  0.76 .\n",
      "The f1 score for buildings  was:  0.25 .\n",
      "The f1 score for clothing  was:  0.30 .\n",
      "The f1 score for cold  was:  0.27 .\n",
      "The f1 score for death  was:  0.42 .\n",
      "The f1 score for direct_report  was:  0.56 .\n",
      "The f1 score for earthquake  was:  0.85 .\n",
      "The f1 score for electricity  was:  0.08 .\n",
      "The f1 score for fire  was:  0.03 .\n",
      "The f1 score for floods  was:  0.66 .\n",
      "The f1 score for food  was:  0.77 .\n",
      "The f1 score for hospitals  was:  0.04 .\n",
      "The f1 score for infrastructure_related  was:  0.01 .\n",
      "The f1 score for medical_help  was:  0.24 .\n",
      "The f1 score for medical_products  was:  0.18 .\n",
      "The f1 score for military  was:  0.26 .\n",
      "The f1 score for missing_people  was:  0.03 .\n",
      "The f1 score for money  was:  0.11 .\n",
      "The f1 score for offer  was:  0.00 .\n",
      "The f1 score for other_aid  was:  0.09 .\n",
      "The f1 score for other_infrastructure  was:  0.00 .\n",
      "The f1 score for other_weather  was:  0.14 .\n",
      "The f1 score for refugees  was:  0.18 .\n",
      "The f1 score for request  was:  0.69 .\n",
      "The f1 score for search_and_rescue  was:  0.07 .\n",
      "The f1 score for security  was:  0.00 .\n",
      "The f1 score for shelter  was:  0.57 .\n",
      "The f1 score for shops  was:  0.00 .\n",
      "The f1 score for storm  was:  0.69 .\n",
      "The f1 score for tools  was:  0.00 .\n",
      "The f1 score for transport  was:  0.28 .\n",
      "The f1 score for water  was:  0.66 .\n",
      "The f1 score for weather_related  was:  0.81 .\n",
      "Mean f1 score:  0.29 Max f1 score: 0.85 Min f1 score: 0.00\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.0,\n",
       " 0.7572684246112239,\n",
       " 0.24605678233438485,\n",
       " 0.30476190476190473,\n",
       " 0.2698412698412698,\n",
       " 0.4181184668989547,\n",
       " 0.5604463732176069,\n",
       " 0.8513800424628449,\n",
       " 0.08000000000000002,\n",
       " 0.03389830508474576,\n",
       " 0.6601941747572815,\n",
       " 0.774757281553398,\n",
       " 0.03773584905660378,\n",
       " 0.006191950464396285,\n",
       " 0.2437137330754352,\n",
       " 0.18367346938775508,\n",
       " 0.2594594594594595,\n",
       " 0.03125,\n",
       " 0.11009174311926605,\n",
       " 0.0,\n",
       " 0.09226594301221167,\n",
       " 0.0,\n",
       " 0.14426229508196722,\n",
       " 0.17801047120418848,\n",
       " 0.6862091938707529,\n",
       " 0.06756756756756757,\n",
       " 0.0,\n",
       " 0.5654135338345865,\n",
       " 0.0,\n",
       " 0.688757396449704,\n",
       " 0.0,\n",
       " 0.27536231884057966,\n",
       " 0.6587771203155818,\n",
       " 0.8056665455866328]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score_labels(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "f1_score with 0 values indicates us that the labels are imbalanced, conducting a grid search will help us get further insights about this behaviour."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The precision score for aid_centers  was:  0.00 .\n",
      "The precision score for aid_related  was:  0.72 .\n",
      "The precision score for buildings  was:  0.76 .\n",
      "The precision score for clothing  was:  0.76 .\n",
      "The precision score for cold  was:  0.89 .\n",
      "The precision score for death  was:  0.80 .\n",
      "The precision score for direct_report  was:  0.72 .\n",
      "The precision score for earthquake  was:  0.90 .\n",
      "The precision score for electricity  was:  0.83 .\n",
      "The precision score for fire  was:  1.00 .\n",
      "The precision score for floods  was:  0.90 .\n",
      "The precision score for food  was:  0.84 .\n",
      "The precision score for hospitals  was:  1.00 .\n",
      "The precision score for infrastructure_related  was:  0.11 .\n",
      "The precision score for medical_help  was:  0.65 .\n",
      "The precision score for medical_products  was:  0.82 .\n",
      "The precision score for military  was:  0.71 .\n",
      "The precision score for missing_people  was:  1.00 .\n",
      "The precision score for money  was:  0.67 .\n",
      "The precision score for offer  was:  0.00 .\n",
      "The precision score for other_aid  was:  0.56 .\n",
      "The precision score for other_infrastructure  was:  0.00 .\n",
      "The precision score for other_weather  was:  0.61 .\n",
      "The precision score for refugees  was:  0.65 .\n",
      "The precision score for request  was:  0.78 .\n",
      "The precision score for search_and_rescue  was:  1.00 .\n",
      "The precision score for security  was:  0.00 .\n",
      "The precision score for shelter  was:  0.78 .\n",
      "The precision score for shops  was:  0.00 .\n",
      "The precision score for storm  was:  0.80 .\n",
      "The precision score for tools  was:  0.00 .\n",
      "The precision score for transport  was:  0.81 .\n",
      "The precision score for water  was:  0.88 .\n",
      "The precision score for weather_related  was:  0.85 .\n",
      "Mean precision score:  0.64 Max precision score: 1.00 Min precision score: 0.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ricardo_2\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\metrics\\_classification.py:1268: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.0,\n",
       " 0.718562874251497,\n",
       " 0.7647058823529411,\n",
       " 0.7619047619047619,\n",
       " 0.8947368421052632,\n",
       " 0.8,\n",
       " 0.7197452229299363,\n",
       " 0.9011235955056179,\n",
       " 0.8333333333333334,\n",
       " 1.0,\n",
       " 0.8986784140969163,\n",
       " 0.8382352941176471,\n",
       " 1.0,\n",
       " 0.1111111111111111,\n",
       " 0.6494845360824743,\n",
       " 0.8181818181818182,\n",
       " 0.7058823529411765,\n",
       " 1.0,\n",
       " 0.6666666666666666,\n",
       " 0.0,\n",
       " 0.5573770491803278,\n",
       " 0.0,\n",
       " 0.6111111111111112,\n",
       " 0.6538461538461539,\n",
       " 0.7767722473604827,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.7800829875518672,\n",
       " 0.0,\n",
       " 0.7972602739726027,\n",
       " 0.0,\n",
       " 0.8085106382978723,\n",
       " 0.8835978835978836,\n",
       " 0.8485080336648814]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "precision_score_labels(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy score for aid_centers  was:  0.98 .\n",
      "The accuracy score for aid_related  was:  0.73 .\n",
      "The accuracy score for buildings  was:  0.94 .\n",
      "The accuracy score for clothing  was:  0.98 .\n",
      "The accuracy score for cold  was:  0.98 .\n",
      "The accuracy score for death  was:  0.96 .\n",
      "The accuracy score for direct_report  was:  0.82 .\n",
      "The accuracy score for earthquake  was:  0.96 .\n",
      "The accuracy score for electricity  was:  0.97 .\n",
      "The accuracy score for fire  was:  0.99 .\n",
      "The accuracy score for floods  was:  0.95 .\n",
      "The accuracy score for food  was:  0.94 .\n",
      "The accuracy score for hospitals  was:  0.99 .\n",
      "The accuracy score for infrastructure_related  was:  0.92 .\n",
      "The accuracy score for medical_help  was:  0.90 .\n",
      "The accuracy score for medical_products  was:  0.94 .\n",
      "The accuracy score for military  was:  0.97 .\n",
      "The accuracy score for missing_people  was:  0.98 .\n",
      "The accuracy score for money  was:  0.98 .\n",
      "The accuracy score for offer  was:  0.99 .\n",
      "The accuracy score for other_aid  was:  0.83 .\n",
      "The accuracy score for other_infrastructure  was:  0.95 .\n",
      "The accuracy score for other_weather  was:  0.93 .\n",
      "The accuracy score for refugees  was:  0.96 .\n",
      "The accuracy score for request  was:  0.88 .\n",
      "The accuracy score for search_and_rescue  was:  0.97 .\n",
      "The accuracy score for security  was:  0.97 .\n",
      "The accuracy score for shelter  was:  0.93 .\n",
      "The accuracy score for shops  was:  0.99 .\n",
      "The accuracy score for storm  was:  0.93 .\n",
      "The accuracy score for tools  was:  0.99 .\n",
      "The accuracy score for transport  was:  0.95 .\n",
      "The accuracy score for water  was:  0.96 .\n",
      "The accuracy score for weather_related  was:  0.87 .\n",
      "Mean accuracy score:  0.94 Max accuracy score: 0.99 Min accuracy score: 0.73\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.9849132511943676,\n",
       " 0.7291928589388986,\n",
       " 0.9399044505908977,\n",
       " 0.981644455619814,\n",
       " 0.976866985164697,\n",
       " 0.9580085491576565,\n",
       " 0.8217249182801106,\n",
       " 0.9647975861201911,\n",
       " 0.9710837314558712,\n",
       " 0.9856675886346492,\n",
       " 0.9471963791802867,\n",
       " 0.9416645712848881,\n",
       " 0.9871762635152125,\n",
       " 0.9192858938898667,\n",
       " 0.9016846869499623,\n",
       " 0.9396530047774705,\n",
       " 0.9655519235604727,\n",
       " 0.9844103595675132,\n",
       " 0.975609756097561,\n",
       " 0.9937138546643198,\n",
       " 0.8317827508171989,\n",
       " 0.9469449333668595,\n",
       " 0.9343726426954991,\n",
       " 0.9605230072919286,\n",
       " 0.8815690218757858,\n",
       " 0.9653004777470455,\n",
       " 0.9746039728438521,\n",
       " 0.9273321599195373,\n",
       " 0.993965300477747,\n",
       " 0.9338697510686447,\n",
       " 0.9934624088508927,\n",
       " 0.9497108373145587,\n",
       " 0.9564998742770933,\n",
       " 0.8654764898164445]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score_labels (y_test,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The recall score for aid_centers  was:  0.00 .\n",
      "The recall score for aid_related  was:  0.80 .\n",
      "The recall score for buildings  was:  0.15 .\n",
      "The recall score for clothing  was:  0.19 .\n",
      "The recall score for cold  was:  0.16 .\n",
      "The recall score for death  was:  0.28 .\n",
      "The recall score for direct_report  was:  0.46 .\n",
      "The recall score for earthquake  was:  0.81 .\n",
      "The recall score for electricity  was:  0.04 .\n",
      "The recall score for fire  was:  0.02 .\n",
      "The recall score for floods  was:  0.52 .\n",
      "The recall score for food  was:  0.72 .\n",
      "The recall score for hospitals  was:  0.02 .\n",
      "The recall score for infrastructure_related  was:  0.00 .\n",
      "The recall score for medical_help  was:  0.15 .\n",
      "The recall score for medical_products  was:  0.10 .\n",
      "The recall score for military  was:  0.16 .\n",
      "The recall score for missing_people  was:  0.02 .\n",
      "The recall score for money  was:  0.06 .\n",
      "The recall score for offer  was:  0.00 .\n",
      "The recall score for other_aid  was:  0.05 .\n",
      "The recall score for other_infrastructure  was:  0.00 .\n",
      "The recall score for other_weather  was:  0.08 .\n",
      "The recall score for refugees  was:  0.10 .\n",
      "The recall score for request  was:  0.61 .\n",
      "The recall score for search_and_rescue  was:  0.03 .\n",
      "The recall score for security  was:  0.00 .\n",
      "The recall score for shelter  was:  0.44 .\n",
      "The recall score for shops  was:  0.00 .\n",
      "The recall score for storm  was:  0.61 .\n",
      "The recall score for tools  was:  0.00 .\n",
      "The recall score for transport  was:  0.17 .\n",
      "The recall score for water  was:  0.53 .\n",
      "The recall score for weather_related  was:  0.77 .\n",
      "Mean recall score:  0.24 Max recall score: 0.81 Min recall score: 0.00\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.0,\n",
       " 0.800381133873273,\n",
       " 0.14661654135338345,\n",
       " 0.19047619047619047,\n",
       " 0.1588785046728972,\n",
       " 0.2830188679245283,\n",
       " 0.4588832487309645,\n",
       " 0.806841046277666,\n",
       " 0.04201680672268908,\n",
       " 0.017241379310344827,\n",
       " 0.5217391304347826,\n",
       " 0.720216606498195,\n",
       " 0.019230769230769232,\n",
       " 0.0031847133757961785,\n",
       " 0.15,\n",
       " 0.10344827586206896,\n",
       " 0.15894039735099338,\n",
       " 0.015873015873015872,\n",
       " 0.06,\n",
       " 0.0,\n",
       " 0.05029585798816568,\n",
       " 0.0,\n",
       " 0.08178438661710037,\n",
       " 0.10303030303030303,\n",
       " 0.6145584725536993,\n",
       " 0.03496503496503497,\n",
       " 0.0,\n",
       " 0.44339622641509435,\n",
       " 0.0,\n",
       " 0.60625,\n",
       " 0.0,\n",
       " 0.16593886462882096,\n",
       " 0.5251572327044025,\n",
       " 0.7669432918395575]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recall_score_labels (y_test,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ricardo_2\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\metrics\\_classification.py:1268: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "cm_y1 = confusion_matrix(y_test.iloc[:,0],y_pred[:,0])\n",
    "cm_y2 = confusion_matrix(y_test.iloc[:,1],y_pred[:,1])\n",
    "\n",
    "\n",
    "\n",
    "cr_y0 = classification_report(y_test.iloc[:,0],y_pred[:,0])\n",
    "cr_y9 = classification_report(y_test.iloc[:,9],y_pred[:,9])\n",
    "cr_y13 = classification_report(y_test.iloc[:,13],y_pred[:,13])\n",
    "cr_y19 = classification_report(y_test.iloc[:,19],y_pred[:,19])\n",
    "cr_y21 = classification_report(y_test.iloc[:,21],y_pred[:,21])\n",
    "cr_y26 = classification_report(y_test.iloc[:,26],y_pred[:,26])\n",
    "cr_y28 = classification_report(y_test.iloc[:,28],y_pred[:,28])\n",
    "cr_y30 = classification_report(y_test.iloc[:,30],y_pred[:,30])\n",
    "cr_y31 = classification_report(y_test.iloc[:,31],y_pred[:,31])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      1.00      0.97      3748\n",
      "           1       0.81      0.17      0.28       229\n",
      "\n",
      "    accuracy                           0.95      3977\n",
      "   macro avg       0.88      0.58      0.62      3977\n",
      "weighted avg       0.94      0.95      0.93      3977\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print (cr_y31)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overall the results look promising. We will now conduct a grid search to check what can be improved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ricardo_2\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\base.py:197: FutureWarning: From version 0.24, get_params will raise an AttributeError if a parameter cannot be retrieved as an instance attribute. Previously it would return None.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "dict_keys(['analyzer', 'binary', 'decode_error', 'dtype', 'encoding', 'input', 'lowercase', 'max_df', 'max_features', 'min_df', 'ngram_range', 'preprocessor', 'stop_words', 'strip_accents', 'token_pattern', 'tokenizer', 'vocabulary'])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CountVectorizer.get_params(CountVectorizer).keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['norm', 'smooth_idf', 'sublinear_tf', 'use_idf'])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TfidfTransformer.get_params(TfidfTransformer).keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['bootstrap', 'ccp_alpha', 'class_weight', 'criterion', 'max_depth', 'max_features', 'max_leaf_nodes', 'max_samples', 'min_impurity_decrease', 'min_impurity_split', 'min_samples_leaf', 'min_samples_split', 'min_weight_fraction_leaf', 'n_estimators', 'n_jobs', 'oob_score', 'random_state', 'verbose', 'warm_start'])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MultiOutputClassifier.get_params(RandomForestClassifier()).keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['cv', 'error_score', 'estimator', 'iid', 'n_jobs', 'param_grid', 'pre_dispatch', 'refit', 'return_train_score', 'scoring', 'verbose'])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "GridSearchCV.get_params(GridSearchCV).keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model():\n",
    "        pipeline = Pipeline([\n",
    "        ('vect',CountVectorizer(tokenizer=tokenize)),\n",
    "        ('tfidf',TfidfTransformer()),\n",
    "        ('clf', MultiOutputClassifier(estimator=RandomForestClassifier()))\n",
    "    ])\n",
    "\n",
    "    # specify parameters for grid search\n",
    "        parameters = {\n",
    "            #'vect__ngram_range': ((1, 1), (1, 2)),\n",
    "            #'vect__max_df': (0.5, 0.75, 1.0),\n",
    "            #'vect__max_features': (None, 5000, 10000),\n",
    "            #'tfidf__use_idf': (True, False),\n",
    "            'clf__estimator__n_estimators': [50, 100],\n",
    "            #'clf__estimator__min_samples_split': [2, 3, 4],\n",
    "            'clf__estimator__random_state': [42]\n",
    "        \n",
    "    }\n",
    "\n",
    "    \n",
    "\n",
    "    # create grid search object\n",
    "        cv = GridSearchCV(pipeline, param_grid=parameters,verbose=1,n_jobs=3)\n",
    "    \n",
    "        return cv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On a sidenote a grid search with more parameters was set, but due to time restrictions, and because our goal isn't a perfect model, it was decided to limit the parameters in order to advance in the project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 2 candidates, totalling 10 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=3)]: Using backend LokyBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  10 out of  10 | elapsed: 18.1min finished\n"
     ]
    }
   ],
   "source": [
    "cv = build_model()\n",
    "cv.fit(X_train, y_train)\n",
    "y_pred = cv.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'clf__estimator__n_estimators': 100, 'clf__estimator__random_state': 42}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the same model that was tested before, and since it was already evaluated, we can proceed with the implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
